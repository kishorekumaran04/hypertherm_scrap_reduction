{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for models and scalers\n",
    "models = {}\n",
    "scalers = {}\n",
    "encoders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [-2147483648, 0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "for strategy in strategies:\n",
    "    # Load the scaler\n",
    "    scaler_path = f'scalers/{strategy}_scaler.pkl'\n",
    "    scalers[strategy] = joblib.load(scaler_path)\n",
    "\n",
    "    model_path = f'models/{strategy}_rf_model.pkl'\n",
    "    models[strategy] = joblib.load(model_path)\n",
    "    \n",
    "    # Load the encoder model\n",
    "    encoder_path = f'encoders/{strategy}_encoder.h5'\n",
    "    encoders[strategy] = load_model(encoder_path)\n",
    "\n",
    "    ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_utilization_all_strategies(new_data):\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        scaler = scalers[strategy]\n",
    "        model = models[strategy]\n",
    "        encoder = encoders[strategy]\n",
    "\n",
    "        # Standardize new data\n",
    "        X_new_scaled = scaler.transform(new_data)\n",
    "\n",
    "        # encode the data\n",
    "        X_new_encoded = encoder.predict(X_new_scaled)\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = model.predict(X_new_encoded)\n",
    "\n",
    "        results[strategy] = prediction\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestdf = pd.read_csv('./Nest.csv')\n",
    "partavgdf = pd.read_csv('./Part_Avg.csv')\n",
    "joined_nest_partsavg_df = pd.merge(nestdf, partavgdf, on='ixJobSummary', how='inner')\n",
    "joined_nest_partsavg_df['CropUtil'] = joined_nest_partsavg_df['dPartArea_Job'] / joined_nest_partsavg_df['dTrueAreaRectified_Job']\n",
    "\n",
    "joined_nest_partsavg_df.drop('dLengthUsed_Avg', axis=1, inplace=True)\n",
    "joined_nest_partsavg_df.drop('dWidthUsed_Avg', axis=1, inplace=True)\n",
    "joined_nest_partsavg_df.drop('dTrueAreaRectified_Job', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "joined_nest_partsavg_df.drop('ixJobSummary', axis=1, inplace=True)\n",
    "X = joined_nest_partsavg_df.drop(columns=['CropUtil', 'fStrategies'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dPartArea_Job              2120.831600\n",
       "dLength_Avg                  13.132457\n",
       "dWidth_Avg                    3.879971\n",
       "dArea_Avg                    19.251057\n",
       "cNested_Avg                  17.000000\n",
       "fExtShape_Avg                 7.000000\n",
       "dExtArea_Avg                 21.953457\n",
       "dExtBoundaryDist_Avg          1.265157\n",
       "dExtContainedDist_Avg        14.416871\n",
       "dLgIntArea_Avg                2.298814\n",
       "dLgIntBoundaryDist_Avg        0.267543\n",
       "dLgIntContainedDist_Avg       1.247286\n",
       "dLgExtConArea_Avg             0.000000\n",
       "dLgExtConBoundaryDist         0.000000\n",
       "dLgExtConContainedDist        0.000000\n",
       "Name: 654237, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[654237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dPartArea_Job              792.805100\n",
       "dLength_Avg                  7.585147\n",
       "dWidth_Avg                   4.883440\n",
       "dArea_Avg                   31.954447\n",
       "cNested_Avg                  4.000000\n",
       "fExtShape_Avg               10.000000\n",
       "dExtArea_Avg                36.932747\n",
       "dExtBoundaryDist_Avg         1.849400\n",
       "dExtContainedDist_Avg        8.584833\n",
       "dLgIntArea_Avg               3.774960\n",
       "dLgIntBoundaryDist_Avg       0.387147\n",
       "dLgIntContainedDist_Avg      1.432107\n",
       "dLgExtConArea_Avg            2.988960\n",
       "dLgExtConBoundaryDist        0.258467\n",
       "dLgExtConContainedDist       1.167313\n",
       "Name: 1201187, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[1201187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dPartArea_Job              4316.764500\n",
       "dLength_Avg                  18.221422\n",
       "dWidth_Avg                   19.914244\n",
       "dArea_Avg                   239.820244\n",
       "cNested_Avg                   1.000000\n",
       "fExtShape_Avg                 0.000000\n",
       "dExtArea_Avg                243.996872\n",
       "dExtBoundaryDist_Avg          7.494961\n",
       "dExtContainedDist_Avg        24.091100\n",
       "dLgIntArea_Avg                4.176633\n",
       "dLgIntBoundaryDist_Avg        0.423661\n",
       "dLgIntContainedDist_Avg       3.188983\n",
       "dLgExtConArea_Avg             8.213883\n",
       "dLgExtConBoundaryDist         0.777067\n",
       "dLgExtConContainedDist        4.030811\n",
       "Name: 345038, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[345038]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New data\n",
    "provided_data = pd.DataFrame({\n",
    "    'dPartArea_Job': [2120.831600, 792.805100, 4316.764500],\n",
    "    'dLength_Avg': [13.132457, 7.585147, 18.221422],\n",
    "    'dWidth_Avg': [3.879971, 4.883440, 19.914244],\n",
    "    'dArea_Avg': [19.251057, 31.954447, 239.820244],\n",
    "    'cNested_Avg': [17.000000, 4.000000, 1.000000],\n",
    "    'fExtShape_Avg': [7.000000, 10.000000, 0.000000],\n",
    "    'dExtArea_Avg': [21.953457, 36.932747, 243.996872],\n",
    "    'dExtBoundaryDist_Avg': [1.265157, 1.849400, 7.494961],\n",
    "    'dExtContainedDist_Avg': [14.416871, 8.584833, 24.091100],\n",
    "    'dLgIntArea_Avg': [2.298814, 3.774960, 4.176633],\n",
    "    'dLgIntBoundaryDist_Avg': [0.267543, 0.387147, 0.423661],\n",
    "    'dLgIntContainedDist_Avg': [1.247286, 1.432107, 3.188983],\n",
    "    'dLgExtConArea_Avg': [0.000000, 2.988960, 8.213883],\n",
    "    'dLgExtConBoundaryDist': [0.000000, 0.258467, 0.777067],\n",
    "    'dLgExtConContainedDist': [0.000000, 1.167313, 4.030811]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Row 1:\n",
      "  Strategy: 16384, Predicted Utilization: 0.6994755071808476\n",
      "  Strategy: 128, Predicted Utilization: 0.6879479545113848\n",
      "  Strategy: 2048, Predicted Utilization: 0.6785828409137011\n",
      "Row 2:\n",
      "  Strategy: 4, Predicted Utilization: 0.6564003193746727\n",
      "  Strategy: 64, Predicted Utilization: 0.6452192235792533\n",
      "  Strategy: 4096, Predicted Utilization: 0.6339417575734481\n",
      "Row 3:\n",
      "  Strategy: 8, Predicted Utilization: 0.7221038533347898\n",
      "  Strategy: 256, Predicted Utilization: 0.7189315839660897\n",
      "  Strategy: 4, Predicted Utilization: 0.718848436045881\n"
     ]
    }
   ],
   "source": [
    "# Predict utilization for the provided data using all strategies\n",
    "pred_util_all_strategies = predict_utilization_all_strategies(provided_data)\n",
    "# for strategy, pred_util in pred_util_all_strategies.items():\n",
    "#     print(f'Predicted utilization for {strategy}: {pred_util}')\n",
    "\n",
    "top_strategies = []\n",
    "for i in range(len(provided_data)):\n",
    "    # Get predictions for the current row across all strategies\n",
    "    row_predictions = {strategy: pred_util[i] for strategy, pred_util in pred_util_all_strategies.items()}\n",
    "    # Sort strategies by predicted utilization\n",
    "    sorted_strategies = sorted(row_predictions.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Get the top 3 strategies\n",
    "    top_3 = sorted_strategies[:3]\n",
    "    top_strategies.append(top_3)\n",
    "\n",
    "# Print the top 3 strategies for each row\n",
    "for idx, top_3 in enumerate(top_strategies):\n",
    "    print(f'Row {idx + 1}:')\n",
    "    for strategy, util in top_3:\n",
    "        print(f'  Strategy: {strategy}, Predicted Utilization: {util}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
